name: CI-CD

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:

env:
  AWS_REGION: ap-south-1
  ECR_REPOSITORY: devops-portfolio-app
  EKS_CLUSTER_NAME: devops-portfolio-eks
  PYTHON_VERSION: "3.11"

jobs:
  pipeline:
    runs-on: ubuntu-latest

    steps:
      # -----------------------------
      # Checkout
      # -----------------------------
      - name: Checkout repository
        uses: actions/checkout@v4

      # -----------------------------
      # Python setup
      # -----------------------------
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # -----------------------------
      # Unit tests (pytest)
      # -----------------------------
      - name: Unit tests (pytest)
        run: |
          export PYTHONPATH="${GITHUB_WORKSPACE}"
          pytest -v

      # -----------------------------
      # Integration tests (docker compose)
      # FIX: wait until /health is ready + show logs on failure
      # -----------------------------
      - name: Integration tests (docker compose)
        run: |
          set -e

          docker compose up -d --build

          echo "Waiting for app health on http://localhost:8000/health ..."
          for i in {1..40}; do
            if curl -fsS http://localhost:8000/health >/dev/null; then
              echo "Health OK ✅"
              break
            fi

            echo "Not ready yet (attempt $i/40) - showing last 20 lines of app logs:"
            docker compose logs --tail=20 app || true
            sleep 3
          done

          # Final check (hard fail if still not ready)
          curl -fsS http://localhost:8000/health

          # If you have your own smoke script, keep it:
          if [ -f "tests/integration/smoke.sh" ]; then
            bash tests/integration/smoke.sh
          fi

          docker compose down

      # -----------------------------
      # Configure AWS credentials (MUST be student-Snir.Amira keys in secrets)
      # -----------------------------
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # -----------------------------
      # Who am I (AWS identity)
      # -----------------------------
      - name: Who am I (AWS identity)
        run: aws sts get-caller-identity

      # -----------------------------
      # Login to ECR
      # -----------------------------
      - name: Login to Amazon ECR
        id: ecr_login
        uses: aws-actions/amazon-ecr-login@v2

      # -----------------------------
      # Build & Push Docker image to ECR
      # -----------------------------
      - name: Build & Push Docker image to ECR
        env:
          ECR_REGISTRY: ${{ steps.ecr_login.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          set -e
          echo "ECR_REGISTRY=$ECR_REGISTRY"
          echo "Building image..."
          docker build -t $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG .
          docker tag $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:latest

          echo "Pushing image..."
          docker push $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG
          docker push $ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:latest

      # -----------------------------
      # Setup kubectl
      # -----------------------------
      - name: Setup kubectl
        run: |
          set -e
          curl -sSL -o kubectl "https://dl.k8s.io/release/v1.31.0/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      # -----------------------------
      # Update kubeconfig for EKS
      # -----------------------------
      - name: Update kubeconfig for EKS
        run: |
          set -e
          aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
          kubectl cluster-info

      # -----------------------------
      # Important: wait for AWS Load Balancer Controller (prevents webhook "no endpoints")
      # -----------------------------
      - name: Wait for aws-load-balancer-controller (if installed)
        run: |
          set -e
          if kubectl -n kube-system get deploy aws-load-balancer-controller >/dev/null 2>&1; then
            echo "Waiting for aws-load-balancer-controller rollout..."
            kubectl -n kube-system rollout status deploy/aws-load-balancer-controller --timeout=5m
            echo "Controller ready ✅"
          else
            echo "aws-load-balancer-controller not found in kube-system (skipping wait)"
          fi

      # -----------------------------
      # Deploy to EKS
      # -----------------------------
      - name: Deploy to EKS
        env:
          ECR_REGISTRY: ${{ steps.ecr_login.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          set -e

          # Apply manifests
          kubectl apply -f k8s-manifests/namespaces/
          kubectl apply -f k8s-manifests/database/
          kubectl apply -f k8s-manifests/app/

          # OPTIONAL (only if you have a deployment you want to force to the new image)
          # Example (change namespace/name/container as needed):
          # kubectl -n demo set image deployment/devops-portfolio-app devops-portfolio-app=$ECR_REGISTRY/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG

          kubectl get pods -A
          kubectl get svc -A
